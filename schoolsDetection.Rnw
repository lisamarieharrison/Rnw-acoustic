\documentclass{article}
 \usepackage{url}
 \usepackage{natbib}
\begin{document}
\SweaveOpts{concordance=TRUE}

\title{Schools detection using the \texttt{R} package \texttt{acoustic} to control Echoview via COM}
\author{Martin J. Cox}
\maketitle
\section{Introduction}
In this vignette we will use the example acoustic data collected during the KOAS voyage to demonstrate how to carry out schools detection using \texttt{acoustic} to control Echoview via COM.  We assume the reader has knowledge of the schools detection algorithm implemented in Echoview.  If not please visit  \url{http://support.echoview.com/WebHelp/Echoview.htm/} to see the Echoview help file and also ...  We also assume the reader is familar with Echoview concepts such as filesets, regions, files and virtual variables.

To run this example, you will need the following files:
\begin{enumerate}
  \item \textbf{Echosounder data:} Simrad EK60 RAW data collected during the KAOS voyage.
  \item \textbf{An Echoview template file:} An Echoview (.EV) file containing the virtual variables used during data processing and must be copied to the Echoview templates folder.
  \item \textbf{Echoview calibration file:} The Echoview .ECS file containing the EK60 calibration parameters.
  \item \textbf{Start and end file regions:} Start and end times of each transect stored as an Echoview region file.
\end{enumerate}
All these files are available from ... and should be downloaded to the data directory specified in the \texttt{dd} object.  \texttt{If you want to reproduce this vignette, you should assign the location of the KAOS example data to \texttt{dd}.}. This vignette requires you to have run the Read Data example first.

First, set the working directory. Echoview requires the full file path to be specified for every file name passed using COM.
<<wd>>=
dd <- 'c:/Users/Lisa/Desktop/KAOS'
@
\section{Loading RAW data into Echoview}
In this section we load the necessary packages into the \texttt{R} workspace, open a COM connection between \texttt{R} and Echoview, then populate the Echoview template file with RAW Simrad EK60 data files.
<<loadlib>>=
library(acoustic)
@
Open a COM connection between \texttt{R} and \texttt{Echoview}:
<<openCOM>>=
EVAppObj <- COMCreate('EchoviewCom.EvApplication')
@
Get a list of the raw files required for each transect:
<<loadTransectDesignator>>=
tF=read.csv(paste(dd,'vignette_file_list_subset.csv', sep = '/'))
head(tF)
tF$filename <- paste(dd,'raw',tF$filename,sep='/')
uniqueTransect <- unique(tF$transectNumber)
@

\section{Add seabed line}
Automatic seabed detection performed poorly so the seabed line was manually edited and here we overwrite an existing editable line and replace it with lines saved in the lines directory of the example data.


\section{Schools detection}
We are now ready to start schools detection. First, we get the ful path and name of all the EV files:
<<EVFileV>>=
fnEVVec <- list.files(dd,full.name=TRUE,pattern=paste("(", "transect", ").*\\.ev$", sep = ""))
@

Schools detection is run on the 120 kHz 7x7 convolution variable separately for each transect. Using a loop, a single transect is opened, processed and closed before moving on to the next transect. For each transect, the regions definitions and Sv values for each aggregation detected are exported.
<<schoolsDetection>>=
  for(i in 1:length(fnEVVec)){
    EVLog <- NULL
    message("Processing ",fnEVVec[i])
    opens <- EVOpenFile(EVAppObj, fileName = fnEVVec[i])
    EVFile <- opens$EVFile
    EVLog <- c(EVLog,opens$msg)
    
    schDet<-EVSchoolsDetect(EVFile = EVFile,
                            acoVarName = '120 7x7 convolution',
                            outputRegionClassName = 'aggregations',
                            deleteExistingRegions = TRUE,
                            distanceMode = "GPS distance",
                            maximumHorizontalLink = 15,#m
                            maximumVerticalLink = 5,#m
                            minimumCandidateHeight = 1,#m
                            minimumCandidateLength = 10,#m
                            minimumSchoolHeight = 2,#m
                            minimumSchoolLength = 15, #m
                            dataThreshold = -70)
    EVLog = c(EVLog, schDet$msg)
    EVLog = c(EVLog, EVSaveFile(EVFile)$msg)
    
    #if aggregations were detected, export the data
    if (schDet$nbrOfDetectedschools > 0) {
    
    #export region definitions all aggregations
    regionClass <- EVRegionClassFinder(EVFile, "aggregations")$regionClass
    EVExportRegionDefByClass(regionClass, paste(dd, "/exported aggregations/aggregation_region_def_transect_", i, ".evr", sep = ""))
    
    #export Sv data for 38kHz and 120kHz all aggregations by region
    EVIntegrationByRegionsExport(EVFile = EVFile, acoVarName = "120 seabed and surface excluded", regionClassName = "aggregations", exportFn = paste(dd, "/exported aggregations/120_aggregation_sv_transect_", i, ".csv", sep= ""))

        EVIntegrationByRegionsExport(EVFile = EVFile, acoVarName = "38 seabed and surface excluded", regionClassName = "aggregations", exportFn = paste(dd, "/exported aggregations/38_aggregation_sv_transect_", i, ".csv", sep= ""))
    
    } 
    
    #close the file
    EVCloseFile(EVFile = EVFile)
  }
@

The exported aggregations are now analysed in R. Firstly, aggregations are subsetted to only include krill using the [1.04, 14.75]dB difference window. Krill aggregations are then clustered using the texxt{ClusterSim} library. These steps are run separately for each transect. It is possible to aggregate all krill aggregations data into one .csv file and run the cluster analysis on the entire survey at once, however this is not demonstrated here.

<<classify, fig.align='center'>>=
library(clusterSim)

  for(i in 1:length(fnEVVec)){

#read in data files - the results of integration by regions for each aggregation

f038 <- read.csv(paste("C:/Users/Lisa/Desktop/KAOS/exported aggregations/38_aggregation_sv_transect_", i, ".csv", sep = ""))
f120 <- read.csv(paste("C:/Users/Lisa/Desktop/KAOS/exported aggregations/120_aggregation_sv_transect_", i, ".csv", sep = ""))
Sv038 <- cbind.data.frame(Region_name = f038$Region_name, Sv038 = f038$Sv_mean)

#merge Sv_mean from 38 to 120 kHz data
ag <- merge(f120, Sv038, by = 'Region_name')

#remove NA values
ag$Sv_mean[ag$Sv_mean == -999] <- NA
ag$ag$Sv038[ag$Sv038 == -999] <- NA

#calculate dB difference and isolate krill swarms
ag$dBdiff <- ag$Sv_mean - ag$Sv038
swarms <- subset(as.matrix(ag), ag$dBdiff > 1.02 & ag$dBdiff < 14.75)

#if no swarms are found, move to the next transect, otherwise run a cluster analysis
if (nrow(swarms) == 0) {
  message("No swarms within difference window of 1.02 - 14.75dB found")
} else {

#select swarm metrics for PCA - there are loads more we could chose, these are just examples
swarms <- swarms[, c("Sv_mean","Sv_max","Sv_min","Corrected_length","Height_mean",
  "Depth_mean","Corrected_thickness","Corrected_perimeter",             
  "Corrected_area","Image_compactness",               
  "Corrected_mean_amplitude","Coefficient_of_variation",
  "Horizontal_roughness_coefficient",
  "Vertical_roughness_coefficient")]

swarms <- apply(swarms, 2, as.numeric)

#scale the data
scaleSwarm <- scale(swarms)

#determine number of clusters in scaled krill swarm data using the gap-stat. see:
#Tibshirani, R., Walther, G., Hastie, T. (2001), Estimating the number of clusters in a data set via the gap statistic, "Journal of the Royal Statistical Society", ser. B, vol. 63, part 2, 411-423.

#the code that follows is lifted from the 
#index.Gap function in clusterSim:
# nc - number_of_clusters

min_nc <- 1
max_nc <- 10
if (nrow(swarms) < 10) {
  max_nc <- nrow(swarms) - 2
}
min    <- 0
clopt  <- NULL

res <- array(0, c(max_nc - min_nc + 1, 2))
res[, 1] <- min_nc:max_nc
found <- FALSE

for (nc in min_nc:max_nc){
  cl1 <- pam(scaleSwarm, nc, diss = FALSE)
  cl2 <- pam(scaleSwarm, nc + 1, diss = FALSE)
  clall <- cbind(cl1$clustering, cl2$clustering)
  gap <- index.Gap(scaleSwarm, clall, B = 20, method = "pam")
  res[nc - min_nc + 1, 2] <- diffu <- gap$diffu
  if ((res[nc-min_nc + 1, 2] >= 0) && (!found)){
    nc1 <- nc
    min <- diffu
    clopt <- cl1$cluster
    found <- TRUE
  }
}
if (found){
  print(paste("Minimal number of clusters where diffu >= 0 is", nc1, "for diffu = ", round(min, 4)), quote= FALSE)
}else{
  print(paste("Transect", i, "I have not found clustering with diffu>=0", quote = FALSE))
}
plot(res, type = "p", pch = 0, xlab = "Number of clusters", ylab = "diffu", xaxt = "n")
abline(h = 0, untf = FALSE)
axis(1, c(min_nc:max_nc))
title(paste("Clustering for transect", i, ""))

swarms <- as.data.frame(swarms)
swarms$type <-  c(pam(scaleSwarm, nc1, diss = FALSE)$clustering)

#print a summary table of the number of swarms assigned to each type
t <- table(swarms$type) #krill swarm types determined by PAM)
print(t)

#the following code can print example results summary for depth, height, length by type, however it is commented due to the large number of tables printed when a large number of clusters are identified
#lapply(split(swarms[, c("Depth_mean" ,'Height_mean','Corrected_length')], swarms$type), summary)
}

message(paste("Finished classifying aggregations for transect", i))
}
@

This vignette has demonstrated an automated method for performing schools detection in texxt{Echoview} and then using texxt{R} to run classification and cluster analysis on the detected aggregations.
\end{document}